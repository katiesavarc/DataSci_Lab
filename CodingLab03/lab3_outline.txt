Our code randomily generates (x,y) coordinates for fake stars and their luminosity. 
In this case we set all stars luminosity to a set value of 1. We plot the 2D coordinates onto a grid, and use a 
histogram to extract the number of stars per bin. Using the number of stars per bin, we
calculate the standard deviation, which we divide by the mean to nomalize the spread to make them 
comparable when using different distances. We then plot this normalized standard deviation as a function of distance and
1/distance. What we expect is that the standard deviation multiplied by the flux (the surface brightness fluctuations) will scale as 1/distance. 

To show that our standard deviation scales as 1/distance we set our initial field of view
as a max distance of 1d and we increment down to 0.5d, calculating a standard deviation for each distance increment.

In final_plot1.png, the last plot shows Sigma_F as a function of distance and 1/distance. We can see that in
the 1/distance subplot the trend is linear where in the distance subplot it is not. This proves that Sigma_F
scales as 1/distance. We also included the histogram figures to show how the bins, as well as number of stars in each bin, change as a function of
distance. The top plot is a simple grid plot where we show the bin
limits as well as the actual data points and where they fall. This shows how the data we generated is
random, and how its disttibution becomes more uniform at larger distances.

Extensions to think about to further this idea: We could have given random luminosities (instead of the set value of 1) to the star to simulate a better
galaxy and represent each star as a point spread function. Our code is set up to be able to implement these changes easily. 

