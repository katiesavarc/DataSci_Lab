Our code randomily generates (x,y) coordinates for fake stars and their luminosity. 
In this case we set all stars luminosity to a set value of 1. We plot the 2D Coordinates
in a histogram so we can extract the number of stars per bin. Using the star density we can 
calculate the standard deviation. What we expect is that the standard deviation will scale as the 
square root of the mean. If each star has a flux, we times the std by this flux and we expect
it to scale 1/distance. 

To show that our standard deviation scales as 1/distance we set our initial field of view
as a max distance of 1 and we scale down to 0.5 and we take multiple data points of std for different 
distances. 

In Finalplot.png the last plot of Sigma_F we plot it against distance and 1/distance. We can see that in
the 1/distance figure the trend is linear where in the distance figure it is not. This proves that Sigma_F
scales as 1/distance. We also included the histogram figures to show how the bins change as a function of
distance as well as how many stars each bin has. The final plot is a simple grid plot where we show the bin
limits as well as the actual data points and where they fall. This shows how the data we generated is uniformly
random.

Extensions to think about to further this idea: We could have given random luminosities to the star to simulate a better
galaxy and represent each star as a point spread function. 

